{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Sigmoid激活函数\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "# Sigmoid激活函数的导数\n",
    "def sigmoid_derivative(x):\n",
    "    return sigmoid(x) * (1 - sigmoid(x))\n",
    "\n",
    "# 均方误差损失函数\n",
    "def mse_loss(y_true, y_pred):\n",
    "    return ((y_true - y_pred) ** 2).mean()\n",
    "\n",
    "# 均方误差损失的导数\n",
    "def mse_loss_derivative(y_true, y_pred):\n",
    "    return -2 * (y_true - y_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 设定初始权重和偏置\n",
    "weights_input_hidden = np.random.normal(0, 1)\n",
    "weights_hidden_output = np.random.normal(0, 1)\n",
    "bias_hidden = np.random.normal(0, 1)\n",
    "bias_output = np.random.normal(0, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 学习率\n",
    "learning_rate = 0.01\n",
    "\n",
    "# 手动实现一个简单的前向传播和反向传播\n",
    "for epoch in range(1000):  # 假设迭代1000次\n",
    "    # 假设输入输出分别为：\n",
    "    x = np.array([0.5])  # 输入\n",
    "    y_true = np.array([0.8])  # 真实输出\n",
    "    \n",
    "    # 前向传播\n",
    "    hidden_layer_input = x * weights_input_hidden + bias_hidden\n",
    "    hidden_layer_output = sigmoid(hidden_layer_input)\n",
    "\n",
    "    output_layer_input = hidden_layer_output * weights_hidden_output + bias_output\n",
    "    y_pred = sigmoid(output_layer_input)\n",
    "    \n",
    "    # 计算损失\n",
    "    loss = mse_loss(y_true, y_pred)\n",
    "    \n",
    "    # 反向传播\n",
    "    # 计算损失函数关于输出层输入的导数\n",
    "    dLoss_dOutputInput = mse_loss_derivative(y_true, y_pred) * sigmoid_derivative(output_layer_input)\n",
    "    \n",
    "    # 从输出层到隐藏层的权重梯度\n",
    "    dLoss_dWeightsHiddenOutput = dLoss_dOutputInput * hidden_layer_output\n",
    "\n",
    "    # 对隐藏层激活函数输入的导数\n",
    "    dLoss_dHiddenInput = dLoss_dOutputInput * weights_hidden_output * sigmoid_derivative(hidden_layer_input)\n",
    "\n",
    "    # 输入层到隐藏层的权重梯度\n",
    "    dLoss_dWeightsInputHidden = dLoss_dHiddenInput * x\n",
    "\n",
    "    # 更新权重和偏置\n",
    "    weights_hidden_output -= learning_rate * dLoss_dWeightsHiddenOutput\n",
    "    bias_output -= learning_rate * dLoss_dOutputInput  # 注意这里用了输出层导数，因为偏置相当于一个权重为1的权重\n",
    "    \n",
    "    weights_input_hidden -= learning_rate * dLoss_dWeightsInputHidden\n",
    "    bias_hidden -= learning_rate * dLoss_dHiddenInput  # 同理，用了隐藏层的导数\n",
    "    \n",
    "    # 每训练100个epochs打印一次损失值\n",
    "    if epoch % 100 == 0:\n",
    "        print(f\"Epoch {epoch}, Loss: {loss}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 0.0022498791804733765\n",
      "Epoch 100, Loss: 0.0018162921684897351\n",
      "Epoch 200, Loss: 0.001474035610810566\n",
      "Epoch 300, Loss: 0.0012018714132563289\n",
      "Epoch 400, Loss: 0.0009840237860927476\n",
      "Epoch 500, Loss: 0.0008086340507822446\n",
      "Epoch 600, Loss: 0.0006666915112733037\n",
      "Epoch 700, Loss: 0.0005512823833137084\n",
      "Epoch 800, Loss: 0.0004570546978996398\n",
      "Epoch 900, Loss: 0.00037983209676369485\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# 定义激活函数和它们的导数\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def sigmoid_derivative(sigmoid_output):\n",
    "    return sigmoid_output * (1 - sigmoid_output)\n",
    "\n",
    "# 定义损失函数及其导数\n",
    "def mse_loss(y_true, y_pred):\n",
    "    return np.mean((y_true - y_pred) ** 2)\n",
    "\n",
    "def mse_loss_derivative(y_true, y_pred):\n",
    "    return -2 * (y_true - y_pred)\n",
    "\n",
    "# 初始化权重和偏置\n",
    "weights_input_hidden = np.random.normal()\n",
    "weights_hidden_output = np.random.normal()\n",
    "bias_hidden = np.random.normal()\n",
    "bias_output = np.random.normal()\n",
    "\n",
    "# 学习率\n",
    "learning_rate = 0.01\n",
    "\n",
    "# 进行前向传播\n",
    "def forward_propagate(x):\n",
    "    # 从输入层到隐藏层\n",
    "    hidden_input = x * weights_input_hidden + bias_hidden\n",
    "    hidden_output = sigmoid(hidden_input)\n",
    "\n",
    "    # 从隐藏层到输出层\n",
    "    output_input = hidden_output * weights_hidden_output + bias_output\n",
    "    y_pred = sigmoid(output_input)\n",
    "\n",
    "    return hidden_output, y_pred\n",
    "\n",
    "# 进行反向传播，并更新权重和偏置\n",
    "def back_propagate(x, y_true, hidden_output, y_pred):\n",
    "    global weights_input_hidden, weights_hidden_output, bias_hidden, bias_output\n",
    "    \n",
    "    # 输出层的梯度\n",
    "    error = mse_loss_derivative(y_true, y_pred)\n",
    "    d_output_input = error * sigmoid_derivative(y_pred)\n",
    "    \n",
    "    # 更新输出层权重和偏置\n",
    "    weights_hidden_output -= learning_rate * (d_output_input * hidden_output)\n",
    "    bias_output -= learning_rate * d_output_input\n",
    "\n",
    "    # 隐藏层的梯度\n",
    "    d_hidden_output = d_output_input * weights_hidden_output\n",
    "    d_hidden_input = d_hidden_output * sigmoid_derivative(hidden_output)\n",
    "\n",
    "    # 更新隐藏层权重和偏置\n",
    "    weights_input_hidden -= learning_rate * (d_hidden_input * x)\n",
    "    bias_hidden -= learning_rate * d_hidden_input\n",
    "\n",
    "# 迭代训练网络\n",
    "def train(x, y_true, epochs):\n",
    "    for epoch in range(epochs):\n",
    "        hidden_output, y_pred = forward_propagate(x)\n",
    "        back_propagate(x, y_true, hidden_output, y_pred)\n",
    "        # 如果是实际环境下，可能需要保存损失值来进行监控\n",
    "        if epoch % 100 == 0:\n",
    "            loss = mse_loss(y_true, y_pred)\n",
    "            print(f\"Epoch {epoch}, Loss: {loss}\")\n",
    "\n",
    "# 训练网络\n",
    "train(x=np.array([0.5]), y_true=np.array([0.8]), epochs=1000)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
