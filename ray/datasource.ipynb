{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyarrow as pa\n",
    "import pyarrow.ipc as ipc\n",
    "import numpy as np\n",
    "import ray\n",
    "\n",
    "ray.init()\n",
    "\n",
    "test_dir = \"./tes\"\n",
    "\n",
    "paths = []\n",
    "\n",
    "for i in range(10):\n",
    "    data = [\n",
    "        pa.array(np.arange(0, i+1, dtype=np.int64)),\n",
    "        pa.array([i] * (i+1), type=pa.int64())\n",
    "    ]\n",
    "    batch = pa.RecordBatch.from_arrays(data, ['numbers', 'strings'])\n",
    "\n",
    "    path = test_dir+\"/\"+str(i)\n",
    "    paths.append(path)\n",
    "    # Serialize RecordBatch to a file\n",
    "    with pa.OSFile(path, 'wb') as sink:\n",
    "        with ipc.new_stream(sink, batch.schema) as writer:\n",
    "            writer.write_batch(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ray\n",
    "\n",
    "# 初始化 Ray\n",
    "ray.init()\n",
    "\n",
    "# 创建一个 Dataset\n",
    "ds = ray.data.range(10)\n",
    "\n",
    "from ray.data.datasource import FilenameProvider\n",
    "class OriFilenameProvider(FilenameProvider):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    def get_filename_for_block(self, block, task_index: int, block_index: int) -> str:\n",
    "        return \"\"\n",
    "from pyarrow import csv\n",
    "# 将 Dataset 写入到 CSV 文件中\n",
    "csv.WriteOptions(include_header=True)\n",
    "ds.write_csv(\"output.csv\",arrow_csv_args_fn=pyarrow..,concurrency=1,filename_provider=OriFilenameProvider(),try_create_dir=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import posixpath\n",
    "\n",
    "a = \"/test/csv.out\"\n",
    "print(posixpath.join(a,\"\"))\n",
    "\n",
    "import pathlib\n",
    "print(pathlib.Path(a).joinpath(\"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ray\n",
    "from ray.data.datasource import Datasource, ReadTask\n",
    "import pyarrow as pa\n",
    "import pyarrow.dataset as ds\n",
    "import pyarrow.ipc as ipc\n",
    "import numpy as np\n",
    "from ray.data.block import BlockMetadata\n",
    "from typing import Iterable\n",
    "from ray.data.block import Block\n",
    "\n",
    "class ReadArrowFilesDatasource(Datasource):\n",
    "    def prepare_read(self, parallelism: int, files):\n",
    "        def read_file(files) -> Iterable[Block]:\n",
    "            for file in files:\n",
    "                with pa.memory_map(file, 'rb') as source:\n",
    "                    with ipc.open_stream(source) as reader:\n",
    "                        tbl = pa.Table.from_batches([b for b in reader])\n",
    "                        print(tbl.to_pandas())\n",
    "                        yield tbl\n",
    "\n",
    "        file_batches = np.array_split(files, np.ceil(len(files) / parallelism))\n",
    "\n",
    "        meta = BlockMetadata(\n",
    "            num_rows=None,\n",
    "            size_bytes=None,\n",
    "            schema=None,\n",
    "            input_files=None,\n",
    "            exec_stats=None,\n",
    "        )\n",
    "        read_tasks = [\n",
    "            ReadTask(lambda p=file_batch: read_file(p), meta)\n",
    "            for file_batch in file_batches\n",
    "        ]\n",
    "        return read_tasks\n",
    "\n",
    "\n",
    "\n",
    "# 创建Dataset\n",
    "custom_datasource = ReadArrowFilesDatasource()\n",
    "\n",
    "dataset = ray.data.read_datasource(custom_datasource, files=paths)\n",
    "dataset._lazy = False\n",
    "# 当你调用 to_arrow_refs 时，每个ObjectRef将对应每个文件中的内容\n",
    "arrow_refs = dataset.to_arrow_refs()\n",
    "\n",
    "\n",
    "print(len(arrow_refs))\n",
    "\n",
    "# for ref in arrow_refs:\n",
    "#     print(ray.get(ref).to_pandas())\n",
    "#     print(\"------------\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dataset.schema())\n",
    "\n",
    "\n",
    "print(dir(dataset.schema()),type(dataset.schema()))\n",
    "for field in dataset.schema().types:\n",
    "    print(field,type(field))\n",
    "print(dataset.schema().base_schema,type(dataset.schema().base_schema))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
